{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f705fe44",
   "metadata": {},
   "source": [
    "# Hybrid Stack Overflow Search with Dense, Sparse and ColBERT Multivectors\n",
    "\n",
    "This notebook implements a production-ready Stack Overflow Q&A search system using:\n",
    "- **Dense vectors** (BAAI/bge-small-en-v1.5) for semantic search\n",
    "- **Sparse vectors** (BM25) for lexical/keyword matching\n",
    "- **ColBERT multivectors** for fine-grained reranking via late interaction\n",
    "### https://www.kaggle.com/datasets/kutayahin/stackoverflow-programming-questions-2020-2025?resource=download\n",
    "Dataset: `stackoverflow_combined.csv`. Pipeline: Hybrid retrieval (dense + sparse with RRF fusion) → ColBERT reranking → Formatted results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f315166a",
   "metadata": {},
   "source": [
    "## Cell 1: Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c635108-94e1-4cad-af48-f2df8eaf41bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"QDRANT_URL\"]=\"\" \n",
    "os.environ[\"QDRANT_API_KEY\"]=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbdbab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies if needed (uncomment for Colab)\n",
    "# !pip install qdrant-client fastembed numpy ranx beautifulsoup4 pandas\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from qdrant_client import QdrantClient, models\n",
    "from fastembed import TextEmbedding, SparseTextEmbedding, LateInteractionTextEmbedding\n",
    "import numpy as np\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Optional: for evaluation\n",
    "try:\n",
    "    import ranx\n",
    "    HAS_RANX = True\n",
    "except ImportError:\n",
    "    HAS_RANX = False\n",
    "\n",
    "CSV_PATH = \"stackoverflow_combined.csv\"\n",
    "\n",
    "def get_qdrant_credentials():\n",
    "    \"\"\"Load Qdrant URL and API key from env or Colab userdata.\"\"\"\n",
    "    try:\n",
    "        return os.getenv(\"QDRANT_URL\"), os.getenv(\"QDRANT_API_KEY\")\n",
    "    except Exception:\n",
    "        print(f\"The keys are {os.getenv(\"QDRANT_URL\"), os.getenv(\"QDRANT_API_KEY\")}\")\n",
    "\n",
    "url, api_key = get_qdrant_credentials()\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b961f8",
   "metadata": {},
   "source": [
    "## Cell 2: Initialize Qdrant Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce6697b",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Qdrant client initialized.\n"
     ]
    }
   ],
   "source": [
    "client = QdrantClient(url=url, api_key=api_key)\n",
    "collection_name = \"stackoverflow_search\"\n",
    "\n",
    "# Optional: delete existing collection for fresh run\n",
    "try:\n",
    "    if client.collection_exists(collection_name):\n",
    "        client.delete_collection(collection_name)\n",
    "        print(f\"Deleted existing collection '{collection_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "\n",
    "print(\"Qdrant client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3396eb",
   "metadata": {},
   "source": [
    "## Cell 3: Collection Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ac6f73",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collection 'stackoverflow_search' created with dense, sparse, and colbert vectors.\n"
     ]
    }
   ],
   "source": [
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config={\n",
    "        \"dense\": models.VectorParams(\n",
    "            size=384,\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "        \"colbert\": models.VectorParams(\n",
    "            size=128,\n",
    "            distance=models.Distance.COSINE,\n",
    "            multivector_config=models.MultiVectorConfig(\n",
    "                comparator=models.MultiVectorComparator.MAX_SIM\n",
    "            ),\n",
    "            hnsw_config=models.HnswConfigDiff(m=0),  # Reranking only, no HNSW\n",
    "        ),\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"sparse\": models.SparseVectorParams(modifier=models.Modifier.IDF),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Payload indexes for faceting (tags and breadcrumbs support filter queries)\n",
    "try:\n",
    "    client.create_payload_index(collection_name, \"tags\", models.PayloadSchemaType.KEYWORD)\n",
    "    client.create_payload_index(collection_name, \"breadcrumbs\", models.PayloadSchemaType.KEYWORD)\n",
    "except Exception as e:\n",
    "    print(f\"Payload index note: {e}\")\n",
    "\n",
    "print(\"Collection 'stackoverflow_search' created with dense, sparse, and colbert vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ff211",
   "metadata": {},
   "source": [
    "## Cell 4: Load Stack Overflow CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe8e7ba6",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Loaded 100 Stack Overflow questions.\n"
     ]
    }
   ],
   "source": [
    "def strip_html(html: str) -> str:\n",
    "    \"\"\"Strip HTML tags from body text.\"\"\"\n",
    "    if pd.isna(html) or not html:\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(str(html), \"html.parser\")\n",
    "    return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "\n",
    "def load_stackoverflow_csv(path: str, max_rows=None) -> list[dict]:\n",
    "    \"\"\"Load Stack Overflow questions from CSV and map to payload schema.\"\"\"\n",
    "    df = pd.read_csv(path, nrows=max_rows)\n",
    "    chunks = []\n",
    "    for _, row in df.iterrows():\n",
    "        qid = row[\"question_id\"]\n",
    "        title = str(row[\"title\"]) if pd.notna(row[\"title\"]) else \"\"\n",
    "        body = strip_html(row[\"body\"])\n",
    "        chunk_text = f\"{title} {body}\".strip()[:4000]  # Truncate for ColBERT token limit\n",
    "\n",
    "        tags_raw = str(row[\"tags\"]) if pd.notna(row[\"tags\"]) else \"\"\n",
    "        tags_list = [t.strip().lower() for t in tags_raw.split(\",\") if t.strip()]\n",
    "        prog_lang = str(row[\"programming_language\"]) if pd.notna(row.get(\"programming_language\")) else \"\"\n",
    "        breadcrumbs = [prog_lang] + tags_list if prog_lang else tags_list\n",
    "\n",
    "        url = f\"https://stackoverflow.com/questions/{qid}\"\n",
    "        chunks.append({\n",
    "            \"question_id\": int(qid),\n",
    "            \"page_title\": title,\n",
    "            \"section_title\": \"Question\",\n",
    "            \"page_url\": url,\n",
    "            \"section_url\": url,\n",
    "            \"breadcrumbs\": breadcrumbs[:10],\n",
    "            \"chunk_text\": chunk_text,\n",
    "            \"prev_section_text\": \"\",\n",
    "            \"next_section_text\": \"\",\n",
    "            \"tags\": tags_list,\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Load Stack Overflow dataset (max_rows limits for faster demo; use None for full dataset)\n",
    "chunks = load_stackoverflow_csv(CSV_PATH, max_rows=100)\n",
    "print(f\"Loaded {len(chunks)} Stack Overflow questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ced320",
   "metadata": {},
   "source": [
    "## Cell 5: Embedding and Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2acf2fce",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Upserted 100 Stack Overflow questions.\n"
     ]
    }
   ],
   "source": [
    "dense_model = TextEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
    "sparse_model = SparseTextEmbedding(\"Qdrant/bm25\", language=\"english\")\n",
    "colbert_model = LateInteractionTextEmbedding(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "texts = [c[\"chunk_text\"] for c in chunks]\n",
    "\n",
    "# Embed dense\n",
    "dense_embeddings = list(dense_model.embed(texts))\n",
    "\n",
    "# Embed sparse (BM25 expects list of indices and values)\n",
    "sparse_embeddings = list(sparse_model.embed(texts))\n",
    "\n",
    "# Embed ColBERT (returns list of arrays, one per doc)\n",
    "colbert_embeddings = list(colbert_model.embed(texts))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "points = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    dense_vec = dense_embeddings[i].tolist()\n",
    "    sparse_vec = models.SparseVector(\n",
    "        indices=sparse_embeddings[i].indices.tolist(),\n",
    "        values=sparse_embeddings[i].values.tolist(),\n",
    "    )\n",
    "    colbert_mat = colbert_embeddings[i]  # shape (num_tokens, 128)\n",
    "    colbert_list = colbert_mat.tolist()\n",
    "\n",
    "    points.append(\n",
    "        models.PointStruct(\n",
    "            id=chunk[\"question_id\"],\n",
    "            vector={\n",
    "                \"dense\": dense_vec,\n",
    "                \"sparse\": sparse_vec,\n",
    "                \"colbert\": colbert_list,\n",
    "            },\n",
    "            payload=chunk,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Batch upsert to avoid timeouts on large datasets\n",
    "for i in range(0, len(points), BATCH_SIZE):\n",
    "    batch = points[i : i + BATCH_SIZE]\n",
    "    client.upsert(collection_name=collection_name, points=batch)\n",
    "print(f\"Upserted {len(points)} Stack Overflow questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3690e",
   "metadata": {},
   "source": [
    "## Cell 6: Search Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a748c2f8",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Query: AttributeError NoneType request AI Tools Gradio\n",
      "Results: 10 points\n"
     ]
    }
   ],
   "source": [
    "def hybrid_search(query: str, limit: int = 10, prefetch_limit: int = 50):\n",
    "    \"\"\"Two-stage search: hybrid (dense+sparse RRF) -> ColBERT rerank.\"\"\"\n",
    "    # Embed query for all three representations\n",
    "    dense_q = next(dense_model.embed([query])).tolist()\n",
    "    sparse_q = next(sparse_model.query_embed([query]))\n",
    "    colbert_q = list(colbert_model.query_embed([query]))[0].tolist()\n",
    "\n",
    "    sparse_query = models.SparseVector(\n",
    "        indices=sparse_q.indices.tolist(),\n",
    "        values=sparse_q.values.tolist(),\n",
    "    )\n",
    "\n",
    "    # Stage 1: Prefetch dense + sparse, fuse with RRF\n",
    "    hybrid_prefetch = models.Prefetch(\n",
    "        prefetch=[\n",
    "            models.Prefetch(query=dense_q, using=\"dense\", limit=prefetch_limit),\n",
    "            models.Prefetch(query=sparse_query, using=\"sparse\", limit=prefetch_limit),\n",
    "        ],\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        limit=prefetch_limit,\n",
    "    )\n",
    "\n",
    "    # Stage 2: Rerank prefetched results with ColBERT multivector\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        prefetch=[hybrid_prefetch],\n",
    "        query=colbert_q,\n",
    "        using=\"colbert\",\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example search\n",
    "query = \"AttributeError NoneType request AI Tools Gradio\"\n",
    "results = hybrid_search(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Results: {len(results.points)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee1cd3",
   "metadata": {},
   "source": [
    "## Cell 7: Result Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ab6a680",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "1. AttributeError: 'NoneType' object has no attribute 'get' in request from AI Tools\n",
      "   URL: https://stackoverflow.com/questions/79810291\n",
      "   Tags: python, artificial-intelligence, huggingface, gradio\n",
      "   Score: 26.74917\n",
      "   Snippet: AttributeError: 'NoneType' object has no attribute 'get' in request from AI Tools I am trying to develop a tool that agent (CodeAgent) will call on user input (prompt) in Gradio. This tool will internally call a REST API to fetch the results. I am getting the error: Code:...\n",
      "\n",
      "2. AttributeError: 'NoneType' object has no attribute 'columns' BPTK-Py\n",
      "   URL: https://stackoverflow.com/questions/79808087\n",
      "   Tags: python, debugging, stack, simulation\n",
      "   Score: 15.661942\n",
      "   Snippet: AttributeError: 'NoneType' object has no attribute 'columns' BPTK-Py First I used the first script and the result was an error,... After I was diagnosed with the second script, it turned out that the value for \"Yearly_Tax_Cost_DM_After_Subsidies\" actually exists. However, I need the first script for...\n",
      "\n",
      "3. AttributeError: module 'mysql.connector' has no attribute 'CMySQLConnection'\n",
      "   URL: https://stackoverflow.com/questions/79809523\n",
      "   Tags: python, mysql-connector-python\n",
      "   Score: 13.641586\n",
      "   Snippet: AttributeError: module 'mysql.connector' has no attribute 'CMySQLConnection' I'm trying to get MySQL connector working in VScode, have used to install it which seems to have worked but whenever I try to use I get AttributeError: module 'mysql.connector' has no attribute 'CMySQLConnection'. How to fi...\n",
      "\n",
      "4. Why does appending data with PySpark raise a \"SQLServerException: CREATE TABLE permission denied\" exception?\n",
      "   URL: https://stackoverflow.com/questions/79807775\n",
      "   Tags: python, sql, sql-server, pyspark, databricks\n",
      "   Score: 10.578304\n",
      "   Snippet: Why does appending data with PySpark raise a \"SQLServerException: CREATE TABLE permission denied\" exception? In my Databricks cluster I'm trying to write a DataFrame to my table with the following code: And this line fails with Py4JJavaError: An error occurred while calling o53884.jdbc. : com.micros...\n",
      "\n",
      "5. Why does a class attribute named `type` make `type[Foo]` fail?\n",
      "   URL: https://stackoverflow.com/questions/79808954\n",
      "   Tags: python, python-typing, mypy\n",
      "   Score: 10.260448\n",
      "   Snippet: Why does a class attribute named `type` make `type[Foo]` fail? When I define a class attribute named , a annotation inside the same class causes to report that the name is a variable and therefore “not valid as a type”. produces : I expected to refer to the built-in , but mypy treats as the class at...\n",
      "\n",
      "6. CatalystAppError: {'code': 'FATAL ERROR', 'message': 'Catalyst headers are empty'} when initializing Catalyst app\n",
      "   URL: https://stackoverflow.com/questions/79810057\n",
      "   Tags: python, zoho, zohocatalyst\n",
      "   Score: 10.194329\n",
      "   Snippet: CatalystAppError: {'code': 'FATAL ERROR', 'message': 'Catalyst headers are empty'} when initializing Catalyst app I’m working on a chatbot project using the Zoho Catalyst SDK. My goal is to use the Catalyst Cache service to store session data for multiple users interacting with my chatbot independen...\n",
      "\n",
      "7. ModuleNotFoundError: No module named 'kiwisolver' -- yet Requirement already satisfied: kiwisolver in /usr/lib/python3/dist-packages (1.3.2)\n",
      "   URL: https://stackoverflow.com/questions/79808094\n",
      "   Tags: python, installation, pip, virtualenv\n",
      "   Score: 10.175929\n",
      "   Snippet: ModuleNotFoundError: No module named 'kiwisolver' -- yet Requirement already satisfied: kiwisolver in /usr/lib/python3/dist-packages (1.3.2) I can run a software from a virtual environment that ends up with the following error: However, if I try to install it I get: What am I missing here?...\n",
      "\n",
      "8. TypeError when calculating loan amount in my Python script\n",
      "   URL: https://stackoverflow.com/questions/79807627\n",
      "   Tags: python, typeerror\n",
      "   Score: 9.209666\n",
      "   Snippet: TypeError when calculating loan amount in my Python script I'm trying to write a python program that will calculate the maximum loan amount based on monthly payment, annual interest rate, and loan duration. Here is my current code : When I run the code I get the following error : How can I fix this ...\n",
      "\n",
      "9. A wrapper route to simulate a FastAPI request\n",
      "   URL: https://stackoverflow.com/questions/79809886\n",
      "   Tags: python, fastapi, middleware, asgi\n",
      "   Score: 8.647156\n",
      "   Snippet: A wrapper route to simulate a FastAPI request My goal is to have a special route which receives name and payload for any other route and then (after doing some unpacking and decoding) call the target route which I prefer to go through HTTP middleware that does validation as if it was requested from ...\n",
      "\n",
      "10. Import error on Flask CRUD App from import Modus from and url_decode\n",
      "   URL: https://stackoverflow.com/questions/79809520\n",
      "   Tags: python, flask\n",
      "   Score: 8.57424\n",
      "   Snippet: Import error on Flask CRUD App from import Modus from and url_decode I am trying to develop a CRUD app using flask. The API is a simple REST API using add, update, delete and show. The update on the route need to use a flask_modus and import Modus since I need to use request method using b\"PATCH\". I...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_result(point, rank: int) -> str:\n",
    "    p = point.payload or {}\n",
    "    title = p.get(\"page_title\", \"N/A\")\n",
    "    url = p.get(\"section_url\", p.get(\"page_url\", \"\"))\n",
    "    snippet = (p.get(\"chunk_text\", \"\") or \"\")[:300]\n",
    "    score = getattr(point, \"score\", None)\n",
    "    tags = p.get(\"tags\", [])\n",
    "    tags_str = \", \".join(tags[:5]) if tags else \"\"\n",
    "    return f\"{rank}. {title}\\n   URL: {url}\\n   Tags: {tags_str}\\n   Score: {score}\\n   Snippet: {snippet}...\"\n",
    "\n",
    "\n",
    "for i, pt in enumerate(results.points, 1):\n",
    "    print(format_result(pt, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a51ead6",
   "metadata": {},
   "source": [
    "## Cell 8: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f899c2d",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Evaluation Results\n",
      "------------------------------------------------------------\n",
      "Query: AttributeError NoneType request AI Tools Gradio...\n",
      "  Recall@10: 1.00, MRR: 1.00, Latency: 144ms\n",
      "Query: Python SSH automation child process TTY...\n",
      "  Recall@10: 1.00, MRR: 1.00, Latency: 125ms\n",
      "Query: FastAPI middleware wrapper route...\n",
      "  Recall@10: 1.00, MRR: 1.00, Latency: 92ms\n",
      "Query: Polars rolling aggregation front date...\n",
      "  Recall@10: 1.00, MRR: 1.00, Latency: 81ms\n",
      "Query: Convert datetime timedelta to int...\n",
      "  Recall@10: 1.00, MRR: 1.00, Latency: 85ms\n",
      "------------------------------------------------------------\n",
      "Aggregate Recall@10: 1.00\n",
      "Aggregate MRR: 1.00\n",
      "P50 Latency: 92ms, P95 Latency: 144ms\n"
     ]
    }
   ],
   "source": [
    "ground_truth_examples = [\n",
    "    {\n",
    "        \"query\": \"AttributeError NoneType request AI Tools Gradio\",\n",
    "        \"expected_ids\": [\"79810291\"],\n",
    "        \"query_type\": \"error\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Python SSH automation child process TTY\",\n",
    "        \"expected_ids\": [\"79809924\"],\n",
    "        \"query_type\": \"how-to\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"FastAPI middleware wrapper route\",\n",
    "        \"expected_ids\": [\"79809886\"],\n",
    "        \"query_type\": \"api\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Polars rolling aggregation front date\",\n",
    "        \"expected_ids\": [\"79809140\"],\n",
    "        \"query_type\": \"how-to\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Convert datetime timedelta to int\",\n",
    "        \"expected_ids\": [\"79809098\"],\n",
    "        \"query_type\": \"how-to\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def extract_question_id(url: str):\n",
    "    \"\"\"Extract question_id from Stack Overflow URL.\"\"\"\n",
    "    if not url:\n",
    "        return None\n",
    "    parts = url.rstrip(\"/\").split(\"/\")\n",
    "    return parts[-1] if parts else None\n",
    "\n",
    "\n",
    "def compute_recall_at_k(retrieved_urls: list[str], expected_ids: list[str], k: int = 10) -> float:\n",
    "    top_k = retrieved_urls[:k]\n",
    "    retrieved_ids = [extract_question_id(u) for u in top_k if extract_question_id(u)]\n",
    "    hits = sum(1 for eid in expected_ids if eid in retrieved_ids)\n",
    "    return 1.0 if hits > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_mrr(retrieved_urls: list[str], expected_ids: list[str]) -> float:\n",
    "    for rank, url in enumerate(retrieved_urls, 1):\n",
    "        qid = extract_question_id(url)\n",
    "        if qid and qid in expected_ids:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "latencies = []\n",
    "eval_results = []\n",
    "\n",
    "for ex in ground_truth_examples:\n",
    "    import time\n",
    "    t0 = time.perf_counter()\n",
    "    res = hybrid_search(ex[\"query\"])\n",
    "    lat = (time.perf_counter() - t0) * 1000\n",
    "    latencies.append(lat)\n",
    "\n",
    "    urls = []\n",
    "    for pt in res.points:\n",
    "        u = (pt.payload or {}).get(\"section_url\") or (pt.payload or {}).get(\"page_url\", \"\")\n",
    "        if u:\n",
    "            urls.append(u)\n",
    "\n",
    "    recall = compute_recall_at_k(urls, ex[\"expected_ids\"])\n",
    "    mrr = compute_mrr(urls, ex[\"expected_ids\"])\n",
    "    eval_results.append({\n",
    "        \"query\": ex[\"query\"],\n",
    "        \"recall@10\": recall,\n",
    "        \"mrr\": mrr,\n",
    "        \"latency_ms\": lat,\n",
    "    })\n",
    "\n",
    "latencies_sorted = sorted(latencies)\n",
    "p50 = latencies_sorted[len(latencies_sorted) // 2] if latencies_sorted else 0\n",
    "p95 = latencies_sorted[int(len(latencies_sorted) * 0.95)] if len(latencies_sorted) > 1 else latencies_sorted[0] if latencies_sorted else 0\n",
    "\n",
    "print(\"Evaluation Results\")\n",
    "print(\"-\" * 60)\n",
    "for r in eval_results:\n",
    "    print(f\"Query: {r['query'][:50]}...\")\n",
    "    print(f\"  Recall@10: {r['recall@10']:.2f}, MRR: {r['mrr']:.2f}, Latency: {r['latency_ms']:.0f}ms\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Aggregate Recall@10: {np.mean([r['recall@10'] for r in eval_results]):.2f}\")\n",
    "print(f\"Aggregate MRR: {np.mean([r['mrr'] for r in eval_results]):.2f}\")\n",
    "print(f\"P50 Latency: {p50:.0f}ms, P95 Latency: {p95:.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f391d9",
   "metadata": {},
   "source": [
    "## Cell 9: Reflection and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf0ae0",
   "metadata": {},
   "source": [
    "## Learnings\n",
    "\n",
    "**High-Level Summary**\n",
    "- **Domain:** Stack Overflow Q&A search (programming questions)\n",
    "- **Key Result:** Hybrid + multivector reranking reached Recall@10=&lt;paste from Evaluation cell&gt; with P95=&lt;paste&gt;ms.\n",
    "\n",
    "**Reproducibility**\n",
    "- **Notebook/App:** `hybrid_docs_search.ipynb`\n",
    "- **Repo (optional):** —\n",
    "- **Models:** dense=`BAAI/bge-small-en-v1.5`, sparse=`Qdrant/bm25` (BM25 + IDF), colbert=`colbert-ir/colbertv2.0`\n",
    "- **Collection:** `stackoverflow_search` (Cosine), points=1000\n",
    "- **Dataset:** 1000 questions from `stackoverflow_combined.csv` (Stack Overflow 2020–2025; snapshot: from file)\n",
    "- **Ground truth:** 5 queries (error / how-to / api / how-to / how-to)\n",
    "\n",
    "**Settings (today)**\n",
    "- **Chunking:** One question per chunk (title + body, HTML stripped; truncation 4000 chars)\n",
    "- **Payload fields:** question_id, page_title, section_title, page_url, section_url, breadcrumbs, tags, chunk_text, prev_section_text, next_section_text\n",
    "- **Fusion:** RRF, k_dense=50, k_sparse=50\n",
    "- **Reranker:** ColBERT (MaxSim), top-k=10\n",
    "- **Index/Search params:** default (hnsw_ef, m, ef_construct not tuned)\n",
    "\n",
    "**Queries (examples)**\n",
    "\n",
    "AttributeError: 'NoneType' object has no attribute 'get' in request from AI Tools\n",
    "   URL: https://stackoverflow.com/questions/79810291\n",
    "   Tags: python, artificial-intelligence, huggingface, gradio\n",
    "   Score: 26.74917\n",
    "   Snippet: AttributeError: 'NoneType' object has no attribute 'get' in request from AI Tools I am trying to develop a tool that agent (CodeAgent) will call on user input (prompt) in Gradio. This tool will internally call a REST API to fetch the results. I am getting the error: Code:...\n",
    "\n",
    "2. AttributeError: 'NoneType' object has no attribute 'columns' BPTK-Py\n",
    "   URL: https://stackoverflow.com/questions/79808087\n",
    "   Tags: python, debugging, stack, simulation\n",
    "   Score: 15.661942\n",
    "   Snippet: AttributeError: 'NoneType' object has no attribute 'columns' BPTK-Py First I used the first script and the result was an error,... After I was diagnosed with the second script, it turned out that the value for \"Yearly_Tax_Cost_DM_After_Subsidies\" actually exists. However, I need the first script for...\n",
    "\n",
    "3. AttributeError: module 'mysql.connector' has no attribute 'CMySQLConnection'\n",
    "   URL: https://stackoverflow.com/questions/79809523\n",
    "   Tags: python, mysql-connector-python\n",
    "   Score: 13.641586\n",
    "   Snippet: AttributeError: module 'mysql.connector' has no attribute 'CMySQLConnection' I'm trying to get MySQL connector working in VScode, have used to install it which seems to have worked but whenever I try to use I get AttributeError: module 'mysql.connector' has no attribute 'CMySQLConnection'. How to fi...\n",
    "\n",
    "4. Why does appending data with PySpark raise a \"SQLServerException: CREATE TABLE permission denied\" exception?\n",
    "   URL: https://stackoverflow.com/questions/79807775\n",
    "   Tags: python, sql, sql-server, pyspark, databricks\n",
    "   Score: 10.578304\n",
    "   Snippet: Why does appending data with PySpark raise a \"SQLServerException: CREATE TABLE permission denied\" exception? In my Databricks cluster I'm trying to write a DataFrame to my table with the following code: And this line fails with Py4JJavaError: An error occurred while calling o53884.jdbc. : com.micros...\n",
    "\n",
    "5. Why does a class attribute named `type` make `type[Foo]` fail?\n",
    "   URL: https://stackoverflow.com/questions/79808954\n",
    "   Tags: python, python-typing, mypy\n",
    "   Score: 10.260448\n",
    "   Snippet: Why does a class attribute named `type` make `type[Foo]` fail? When I define a class attribute named , a annotation inside the same class causes to report that the name is a variable and therefore “not valid as a type”. produces : I expected to refer to the built-in , but mypy treats as the class at...\n",
    "\n",
    "6. CatalystAppError: {'code': 'FATAL ERROR', 'message': 'Catalyst headers are empty'} when initializing Catalyst app\n",
    "   URL: https://stackoverflow.com/questions/79810057\n",
    "   Tags: python, zoho, zohocatalyst\n",
    "   Score: 10.194329\n",
    "   Snippet: CatalystAppError: {'code': 'FATAL ERROR', 'message': 'Catalyst headers are empty'} when initializing Catalyst app I’m working on a chatbot project using the Zoho Catalyst SDK. My goal is to use the Catalyst Cache service to store session data for multiple users interacting with my chatbot independen...\n",
    "\n",
    "7. ModuleNotFoundError: No module named 'kiwisolver' -- yet Requirement already satisfied: kiwisolver in /usr/lib/python3/dist-packages (1.3.2)\n",
    "   URL: https://stackoverflow.com/questions/79808094\n",
    "   Tags: python, installation, pip, virtualenv\n",
    "   Score: 10.175929\n",
    "   Snippet: ModuleNotFoundError: No module named 'kiwisolver' -- yet Requirement already satisfied: kiwisolver in /usr/lib/python3/dist-packages (1.3.2) I can run a software from a virtual environment that ends up with the following error: However, if I try to install it I get: What am I missing here?...\n",
    "\n",
    "8. TypeError when calculating loan amount in my Python script\n",
    "   URL: https://stackoverflow.com/questions/79807627\n",
    "   Tags: python, typeerror\n",
    "   Score: 9.209666\n",
    "   Snippet: TypeError when calculating loan amount in my Python script I'm trying to write a python program that will calculate the maximum loan amount based on monthly payment, annual interest rate, and loan duration. Here is my current code : When I run the code I get the following error : How can I fix this ...\n",
    "\n",
    "9. A wrapper route to simulate a FastAPI request\n",
    "   URL: https://stackoverflow.com/questions/79809886\n",
    "   Tags: python, fastapi, middleware, asgi\n",
    "   Score: 8.647156\n",
    "   Snippet: A wrapper route to simulate a FastAPI request My goal is to have a special route which receives name and payload for any other route and then (after doing some unpacking and decoding) call the target route which I prefer to go through HTTP middleware that does validation as if it was requested from ...\n",
    "\n",
    "10. Import error on Flask CRUD App from import Modus from and url_decode\n",
    "   URL: https://stackoverflow.com/questions/79809520\n",
    "   Tags: python, flask\n",
    "   Score: 8.57424\n",
    "   Snippet: Import error on Flask CRUD App from import Modus from and url_decode I am trying to develop a CRUD app using flask. The API is a simple REST API using add, update, delete and show. The update on the route need to use a flask_modus and import Modus since I need to use request method using b\"PATCH\". I...\n",
    "\n",
    "**Evaluation**\n",
    "- Recall@10: &lt;paste&gt; | MRR: &lt;paste&gt; | P50: &lt;paste&gt;ms | P95: &lt;paste&gt;ms  \n",
    "### Evaluation Results\n",
    "\n",
    "Query: AttributeError NoneType request AI Tools Gradio...\n",
    "  Recall@10: 1.00, MRR: 1.00, Latency: 144ms\n",
    "Query: Python SSH automation child process TTY...\n",
    "  Recall@10: 1.00, MRR: 1.00, Latency: 125ms\n",
    "Query: FastAPI middleware wrapper route...\n",
    "  Recall@10: 1.00, MRR: 1.00, Latency: 92ms\n",
    "Query: Polars rolling aggregation front date...\n",
    "  Recall@10: 1.00, MRR: 1.00, Latency: 81ms\n",
    "Query: Convert datetime timedelta to int...\n",
    "  Recall@10: 1.00, MRR: 1.00, Latency: 85ms\n",
    "Aggregate Recall@10: 1.00\n",
    "Aggregate MRR: 1.00\n",
    "P50 Latency: 92ms, P95 Latency: 144ms \n",
    "\n",
    "**Why these matched**\n",
    "- Dense embeddings capture semantic similarity (e.g. “AttributeError” / “request” / “Gradio”); sparse BM25 hits exact terms and tags; ColBERT reranks by token-level overlap so the intended question surfaces in the top 10.\n",
    "\n",
    "**Surprise**\n",
    "- \"ColBERT reranking can noticeably reorder the RRF list; some queries that rank mid-list after fusion jump to #1 after MaxSim.\"\n",
    "\n",
    "**Next step**\n",
    "- \"Run with full dataset (max_rows=None), tune prefetch_limit (50→100→200), and compare RRF vs DBSF to see impact on Recall@10 and latency.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78fc050-031a-4754-91c0-1086f0cee054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
